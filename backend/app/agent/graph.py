# backend/app/agent/graph.py

import operator
import json # For parsing simulated LLM tool call JSON
from typing import TypedDict, Annotated, List, Optional, Dict, Any
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage, SystemMessage
from langgraph.graph import StateGraph, END

# Assuming get_llm_response will be the actual LLM service call
# from backend.app.services.llm_service import get_llm_response, _convert_lc_messages_to_vertex_content
# For now, we'll simulate its behavior.
from backend.app.agent.constants import AGENT_PERSONA_LOGOS_COMPASSIONATE_PROFESSIONAL, AGENT_TONE_RESPECTFUL_COLLABORATIVE_DEFERENTIAL

# --- Tool Definition (Placeholder) ---
RETRIEVE_KNOWLEDGE_BASE_TOOL_DEFINITION = {
    "name": "retrieve_knowledge_base",
    "description": (
        "Searches and retrieves information from the curated clinical knowledge base. "
        "Use this for questions about medical conditions, protocols, drug information, "
        "clinical guidelines, or similar factual inquiries."
    ),
    "parameters": {
        "type": "object",
        "properties": {
            "query": {
                "type": "string",
                "description": "The specific question or search terms to look up in the knowledge base. Optimize for keyword-based search."
            }
        },
        "required": ["query"]
    }
}

# --- AgentState Definition ---
class AgentState(TypedDict):
    user_query: str
    conversation_history: Annotated[List[BaseMessage], operator.add]
    conversation_history_string: str # For easy inclusion in prompts

    # Output from llm_reasoner_node:
    llm_reasoner_decision: Optional[str] # "tool_call" or "direct_response"
    llm_reasoner_response_text: Optional[str] # Text if direct_response
    # Parsed tool calls from LLM, matching Vertex AI's expected input for tool execution.
    # This is a list of dicts, where each dict is like:
    # {'functionCall': {'name': 'tool_name', 'args': {'arg1': 'value1'}}}
    # For LangGraph, we might adapt this to a simpler list of {'name': ..., 'args': ..., 'id': ...}
    tool_calls_generated: Optional[List[Dict[str, Any]]]

    # Output from tool_executor_node:
    # List of ToolMessage-like objects or dicts to be added to history
    executed_tool_responses_for_history: Optional[List[Dict[str, Any]]]
    rag_context: Optional[str] # Specific context from retrieve_knowledge_base

    # Output from rag_synthesis_node or final response:
    final_response: Optional[str]


# --- Simulated LLM Service Call ---
async def simulate_get_llm_response(
    system_prompt_text: str,
    history_plus_prompt_vertex_content: List[Dict[str, Any]], # Simplified Vertex Content
    tools_schema_for_llm: Optional[List[Dict[str, Any]]] = None,
    decision_type: str = "direct_response", # "tool_call" or "direct_response"
    tool_call_query: Optional[str] = "simulated query"
) -> Dict[str, Any]: # Simulates structure of LLM output (text or tool call)
    """
    Simulates the behavior of `get_llm_response` from `llm_service.py`.
    Returns a dictionary that mimics the structure of a parsed LLM response,
    either a text response or a tool call.
    """
    print(f"--- SIMULATING LLM CALL ---")
    print(f"System Prompt: {system_prompt_text[:100]}...")
    # print(f"History + Prompt Content: {history_plus_prompt_vertex_content}")
    # print(f"Tools Schema: {tools_schema_for_llm}")

    if tools_schema_for_llm and decision_type == "tool_call":
        # Simulate LLM deciding to call the retrieve_knowledge_base tool
        print(f"--- SIMULATING LLM: Decided to CALL TOOL ---")
        # This is the format the Vertex AI SDK would return for a function call
        # The actual 'id' for the tool call would be generated by the LLM or SDK.
        # For Langchain, AIMessage expects tool_calls as List[ToolCall(name, args, id)]
        # Let's simulate the raw output that graph.py logic would then parse.
        return {
            "text": None, # Or could be reasoning text like "I need to find information about..."
            "tool_calls": [ # This matches more closely what Langchain AIMessage expects for tool_calls
                {
                    "name": RETRIEVE_KNOWLEDGE_BASE_TOOL_DEFINITION["name"],
                    "args": {"query": tool_call_query if tool_call_query else "Simulated knowledge query"},
                    "id": "simulated_tool_call_id_001" # Important for matching response
                }
            ]
        }
    else:
        # Simulate LLM generating a direct text response
        print(f"--- SIMULATING LLM: Decided to RESPOND DIRECTLY/SYNTHESIZE ---")
        # For synthesis, the system_prompt_text would be the synthesis prompt.
        # The history_plus_prompt_vertex_content would include user query, history, and RAG context.
        simulated_text = f"Simulated LLM text response based on: {history_plus_prompt_vertex_content[-1]['parts'][0]['text'][:50]}..."
        if "Information Retrieved from Knowledge Base" in system_prompt_text:
            simulated_text = f"Synthesized answer: {history_plus_prompt_vertex_content[-1]['parts'][0]['text'][:50]}... with RAG."

        return {"text": simulated_text, "tool_calls": None}

# --- Helper to convert LC messages to simplified Vertex Content for simulation ---
def _convert_lc_messages_to_sim_vertex_content(
    lc_messages: Optional[List[BaseMessage]],
    current_prompt_text: Optional[str] = None
) -> List[Dict[str, Any]]:
    contents = []
    if lc_messages:
        for msg in lc_messages:
            if isinstance(msg, HumanMessage):
                contents.append({"role": "user", "parts": [{"text": msg.content}]})
            elif isinstance(msg, AIMessage):
                # Simplified: just taking text content for simulation
                if msg.content:
                    contents.append({"role": "model", "parts": [{"text": msg.content}]})
                # In real conversion, tool_calls and tool_responses on AIMessage are handled
            elif isinstance(msg, ToolMessage):
                 contents.append({"role": "function", "parts": [{"functionResponse": {"name": msg.name, "response": {"content": msg.content}}}]})

    if current_prompt_text:
        contents.append({"role": "user", "parts": [{"text": current_prompt_text}]})
    return contents

# --- Node Functions ---
async def llm_reasoner_node(state: AgentState):
    print(f"\n--- NODE: llm_reasoner_node ---")
    current_user_query = state['user_query']
    history = state.get('conversation_history', [])

    conversation_history_string = "\n".join(
        [f"{(msg.type).upper()}: {msg.content}" for msg in history]
    )
    # state['conversation_history_string'] = conversation_history_string # Ensure it's in state
    # This modification ^ is problematic for LangGraph as it tries to update state in place.
    # Instead, nodes should return all state fields they intend to modify.
    # For now, conversation_history_string is constructed and used locally.
    # If it needs to be passed to subsequent nodes via state, it should be in the return dict.

    system_message_reasoner_rag = f"""
You are Noah.AI, an AI assistant for nurses, operating under the principles of the Logos Accord.
Your persona is {AGENT_PERSONA_LOGOS_COMPASSIONATE_PROFESSIONAL} and your tone is {AGENT_TONE_RESPECTFUL_COLLABORATIVE_DEFERENTIAL}.
You are tasked with assisting the user by answering questions and performing tasks based on the provided context and available tools.

**Current Conversation History:**
<conversation_history>
{conversation_history_string}
</conversation_history>

**User Query:**
<user_query>
{current_user_query}
</user_query>

**Available Tools:**
You have access to the following tool:
- **`{RETRIEVE_KNOWLEDGE_BASE_TOOL_DEFINITION['name']}`**:
    - Description: {RETRIEVE_KNOWLEDGE_BASE_TOOL_DEFINITION['description']}
    - Arguments: {RETRIEVE_KNOWLEDGE_BASE_TOOL_DEFINITION['parameters']}

**Your Task (Reasoning & Tool Call Generation):**
1. Analyze the <user_query> in the context of the <conversation_history>.
2. **Tool Usage Decision (ReAct):**
    * If the <user_query> would benefit from the `{RETRIEVE_KNOWLEDGE_BASE_TOOL_DEFINITION['name']}` tool, state your reasoning and then generate the tool call.
    * Otherwise, explain briefly and formulate a direct response.
3. **Query Formulation (if using tool):**
    * Formulate a concise, keyword-focused `query` for the tool.
4. **Output Format:**
    * Tool Call: JSON like `{{"tool_calls": [{{"name": "{RETRIEVE_KNOWLEDGE_BASE_TOOL_DEFINITION['name']}", "args": {{"query": "..."}}}}]}}`
    * Direct Response: Plain text.
"""
    tools_schema_for_llm = [RETRIEVE_KNOWLEDGE_BASE_TOOL_DEFINITION]
    history_plus_prompt_sim_vertex_content = _convert_lc_messages_to_sim_vertex_content(history, current_user_query)

    sim_decision = "tool_call" if "symptoms" in current_user_query.lower() or "protocol" in current_user_query.lower() else "direct_response"
    sim_tool_query = f"info on {current_user_query}" if sim_decision == "tool_call" else None

    llm_output_simulated = await simulate_get_llm_response(
        system_prompt_text=system_message_reasoner_rag,
        history_plus_prompt_vertex_content=history_plus_prompt_sim_vertex_content,
        tools_schema_for_llm=tools_schema_for_llm,
        decision_type=sim_decision,
        tool_call_query=sim_tool_query
    )

    generated_tool_calls = llm_output_simulated.get("tool_calls")
    direct_text_response = llm_output_simulated.get("text")

    return_state_update = {"conversation_history_string": conversation_history_string}


    if generated_tool_calls:
        print(f"LLM Reasoner Output: TOOL CALL - {generated_tool_calls}")
        return_state_update.update({
            "llm_reasoner_decision": "tool_call",
            "tool_calls_generated": generated_tool_calls,
            "llm_reasoner_response_text": None
        })
    else:
        print(f"LLM Reasoner Output: DIRECT RESPONSE - {direct_text_response}")
        return_state_update.update({
            "llm_reasoner_decision": "direct_response",
            "llm_reasoner_response_text": direct_text_response,
            "tool_calls_generated": None
        })
    return return_state_update

async def tool_executor_node(state: AgentState):
    print(f"\n--- NODE: tool_executor_node ---")
    tool_calls_to_execute = state.get("tool_calls_generated")

    if not tool_calls_to_execute:
        print("No tool calls to execute.")
        return {"executed_tool_responses_for_history": [], "rag_context": None}

    tool_responses_for_history = []
    current_rag_context = None

    for tool_call in tool_calls_to_execute:
        tool_name = tool_call.get("name")
        tool_args = tool_call.get("args", {})
        tool_call_id = tool_call.get("id", "unknown_tool_call_id")

        print(f"Executing tool: {tool_name} with args: {tool_args} (Call ID: {tool_call_id})")

        if tool_name == RETRIEVE_KNOWLEDGE_BASE_TOOL_DEFINITION["name"]:
            query = tool_args.get("query", "")
            simulated_rag_output = f"Simulated RAG context for query '{query}': Symptoms include sweating, confusion, and tremors. Source: Clinical Guideline XYZ."
            print(f"Tool '{tool_name}' simulated output: {simulated_rag_output[:100]}...")
            current_rag_context = simulated_rag_output
            tool_responses_for_history.append({
                "tool_call_id": tool_call_id,
                "name": tool_name,
                "content": current_rag_context
            })
        else:
            print(f"Unknown tool: {tool_name}")
            tool_responses_for_history.append({
                "tool_call_id": tool_call_id,
                "name": tool_name,
                "content": f"Error: Tool '{tool_name}' not found or simulated."
            })

    return {
        "executed_tool_responses_for_history": tool_responses_for_history,
        "rag_context": current_rag_context
    }

async def rag_synthesis_node(state: AgentState):
    print(f"\n--- NODE: rag_synthesis_node ---")
    user_query = state['user_query']
    conversation_history_string = state.get('conversation_history_string', "") # Should be set by reasoner
    rag_context = state.get('rag_context', "No specific information was found in the knowledge base.")
    history = state.get('conversation_history', [])

    system_message_synthesis_rag = f"""
You are Noah.AI, an AI assistant for nurses, operating under the principles of the Logos Accord.
Your persona is {AGENT_PERSONA_LOGOS_COMPASSIONATE_PROFESSIONAL} and your tone is {AGENT_TONE_RESPECTFUL_COLLABORATIVE_DEFERENTIAL}.
You have received information from the knowledge base to help answer the user's query.

**Original User Query:**
<user_query>
{user_query}
</user_query>

**Conversation History (Context):**
<conversation_history>
{conversation_history_string}
</conversation_history>

**Information Retrieved from Knowledge Base (`rag_context`):**
<rag_context>
{rag_context}
</rag_context>

**Your Task (Synthesize and Respond):**
1. Review all information.
2. Synthesize a comprehensive answer to the <user_query> based PRIMARILY on <rag_context>.
3. **`ALETHIA_FIDELITY_CONSTRAINT`**: Ground answer in <rag_context>. Attribute ("The knowledge base states...").
4. If <rag_context> is insufficient, state that clearly. DO NOT FABRICATE.
5. Output: Plain text response.
"""
    synthesis_input_sim_vertex_content = _convert_lc_messages_to_sim_vertex_content(history, user_query)

    llm_output_simulated = await simulate_get_llm_response(
        system_prompt_text=system_message_synthesis_rag,
        history_plus_prompt_vertex_content=synthesis_input_sim_vertex_content,
        tools_schema_for_llm=None,
        decision_type="synthesis_response"
    )

    final_response_text = llm_output_simulated.get("text", "Error: Could not synthesize response.")
    print(f"RAG Synthesis Output: {final_response_text}")

    return {"final_response": final_response_text}

# --- Conditional Edges ---
def should_continue_to_tool_execution(state: AgentState) -> str:
    print(f"\n--- CONDITIONAL EDGE: should_continue_to_tool_execution ---")
    if state.get("llm_reasoner_decision") == "tool_call" and state.get("tool_calls_generated"):
        print("Decision: ROUTE to tool_executor_node")
        return "execute_tool"
    else:
        # If it's a direct response, the text is in 'llm_reasoner_response_text'.
        # This text should become the 'final_response'.
        # We can achieve this by routing to a small node that transfers the value,
        # or by ensuring the 'END' condition correctly uses this field.
        # For now, we will assume that if the graph ends and 'final_response' is not set,
        # 'llm_reasoner_response_text' will be used. The run_agent_test logic handles this.
        print("Decision: ROUTE to END (direct response)")
        return "end_direct_response"


# --- Graph Definition ---
def build_agent_workflow():
    workflow = StateGraph(AgentState)

    workflow.add_node("llm_reasoner", llm_reasoner_node)
    workflow.add_node("tool_executor", tool_executor_node)
    workflow.add_node("rag_synthesis", rag_synthesis_node)

    workflow.set_entry_point("llm_reasoner")

    workflow.add_conditional_edges(
        "llm_reasoner",
        should_continue_to_tool_execution,
        {
            "execute_tool": "tool_executor",
            "end_direct_response": END
        }
    )
    workflow.add_edge("tool_executor", "rag_synthesis")
    workflow.add_edge("rag_synthesis", END)

    app = workflow.compile()
    return app

# --- Main execution (for testing) ---
async def run_agent_test(user_input: str, initial_history: Optional[List[BaseMessage]] = None):
    print(f"\n\n--- Running Agent Test for: '{user_input}' ---")
    agent_app = build_agent_workflow()

    current_history = list(initial_history) if initial_history else []

    # Append current user query to history for this run
    # This is tricky. The graph expects `conversation_history` to be the state *before* the current query.
    # The `user_query` is the current turn.
    # So, `current_history` here is the history *before* this `user_input`.

    graph_inputs = {
        "user_query": user_input,
        "conversation_history": current_history # History *before* this user_input
    }

    # Add user's current message to history for the next turn's context
    # This must be done *after* the graph runs with the previous history state
    updated_history_for_next_turn = list(current_history) # Copy
    updated_history_for_next_turn.append(HumanMessage(content=user_input))


    final_state = await agent_app.ainvoke(graph_inputs)

    print("\n--- AGENT RUN FINAL STATE ---")
    # print(json.dumps(final_state, indent=2, default=str))

    response_to_user = ""
    if final_state.get("final_response"):
        response_to_user = final_state["final_response"]
        ai_response_message = AIMessage(content=response_to_user)
        updated_history_for_next_turn.append(ai_response_message)

    elif final_state.get("llm_reasoner_response_text"):
        response_to_user = final_state["llm_reasoner_response_text"]
        ai_response_message = AIMessage(content=response_to_user)
        updated_history_for_next_turn.append(ai_response_message)

    else:
        response_to_user = "I'm sorry, I encountered an issue and couldn't process your request."
        updated_history_for_next_turn.append(AIMessage(content=response_to_user))

    # If tool usage happened, correctly form AIMessage with tool_calls and add ToolMessages
    if final_state.get("tool_calls_generated") and final_state.get("executed_tool_responses_for_history"):
        # The AIMessage that initiated the tool call.
        # Its content might be reasoning or empty if LLM directly outputs tool_calls.
        # For simulation, llm_reasoner_node doesn't put reasoning text in AIMessage content if tool call.
        ai_message_with_tool_calls = AIMessage(
            content=final_state.get("llm_reasoner_response_text") or "", # Usually empty if tool_calls present
            tool_calls=final_state["tool_calls_generated"]
        )
        # Replace the simple AIMessage (if it was added above for direct response path)
        # This logic is getting complex for a simple test runner; a proper conversational manager is needed.
        # For now, if a tool call happened, the 'response_to_user' is from synthesis.
        # The history should reflect: HumanIn, AIMessage(tool_calls), ToolMessages..., AIMessage(synthesis)

        # Let's adjust history construction:
        # 1. HumanMessage (already added to updated_history_for_next_turn)
        # 2. AIMessage (with tool calls, if any)
        # 3. ToolMessages (if any)
        # 4. AIMessage (with final synthesized response, if RAG path) OR AIMessage (direct response)

        # Correcting history logic:
        # Remove the simple AIMessage if we are about to add a more complex sequence
        if updated_history_for_next_turn and isinstance(updated_history_for_next_turn[-1], AIMessage) and \
           (updated_history_for_next_turn[-1].content == response_to_user or response_to_user in updated_history_for_next_turn[-1].content) : # Imperfect check
            # This logic is flawed. Let's reconstruct carefully.
            pass # Will be handled by appending the sequence below.
            # The issue is that `response_to_user` is already the *final* response.
            # The AIMessage that *caused* the tool call is different.

        # If a tool was called, the `llm_reasoner_node` would have produced `tool_calls_generated`.
        # The `AIMessage` that included these tool calls is what we need to add.
        # Then the `ToolMessage`s, then the `AIMessage` from `rag_synthesis_node`.

        # Simplified history update for now:
        # The `final_state` has `conversation_history` which *includes* the sequence of Human, AI(tool_call), Tool, AI(response)
        # if the graph is set up with `Annotated[List[BaseMessage], operator.add]` for `conversation_history`.
        # Let's check if LangGraph's state `conversation_history` is what we need.

        # The `final_state['conversation_history']` should have the full sequence if nodes correctly return it.
        # Our nodes currently don't explicitly return 'conversation_history'. LangGraph appends based on `operator.add`.
        # Let's print the history from final_state to see.
        # print("--- Final State Conversation History ---")
        # for msg in final_state.get('conversation_history', []):
        #     print(f"  {msg.type.upper()}: {msg.content}" + (f" (Tool Calls: {msg.tool_calls})" if isinstance(msg, AIMessage) and hasattr(msg, 'tool_calls') and msg.tool_calls else ""))


    print(f"\nResponse to User: {response_to_user}")
    print("--- Updated History for Next Turn (derived from final_state['conversation_history']) ---")
    # The `final_state['conversation_history']` should be the most accurate.
    # It will contain the initial history + the messages from the current run.
    # The HumanMessage for the *current* input is part of this.
    for msg_idx, msg in enumerate(final_state.get('conversation_history', [])):
        tool_call_info = ""
        if isinstance(msg, AIMessage) and hasattr(msg, 'tool_calls') and msg.tool_calls:
            tool_call_info = f" (Tool Calls: {msg.tool_calls})"
        elif isinstance(msg, AIMessage) and hasattr(msg, 'invalid_tool_calls') and msg.invalid_tool_calls: # LangGraph might use this
             tool_call_info = f" (Invalid Tool Calls: {msg.invalid_tool_calls})"

        tool_message_info = ""
        if isinstance(msg, ToolMessage):
            tool_message_info = f" (ID: {msg.tool_call_id}, Name: {msg.name})"

        print(f"  [{msg_idx}] {msg.type.upper()}: {msg.content}{tool_call_info}{tool_message_info}")

    return final_state.get('conversation_history', []) # Return the full history for the next turn


async def main_test_suite():
    # Test 1: Query that should trigger RAG
    # History starts empty
    updated_history = await run_agent_test("What are the symptoms of hypoglycemia?", [])

    print("\n" + "="*50 + "\n")

    # Test 2: Follow-up or simple query that should be a direct response
    # History from previous turn is passed
    updated_history = await run_agent_test("Thanks, that was helpful!", updated_history)

    print("\n" + "="*50 + "\n")

    # Test 3: Another query that should trigger RAG, using the latest history
    updated_history = await run_agent_test("What is the protocol for sepsis?", updated_history)


if __name__ == "__main__":
    import asyncio
    asyncio.run(main_test_suite())
