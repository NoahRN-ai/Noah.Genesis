# RAG Service (`rag_service.py`)

This document describes the Retrieval Augmented Generation (RAG) service for Project Noah MVP. This service is responsible for fetching relevant text chunks from a knowledge base, which is indexed in Vertex AI Vector Search.

## 1. Overview

The `rag_service.py` module provides a core function, `retrieve_relevant_chunks`, which:
1.  Takes a user's query text.
2.  Generates a vector embedding for this query using the same Vertex AI embedding model used during a RAG pipeline (`textembedding-gecko@003`).
3.  Queries a deployed Vertex AI Vector Search Index Endpoint to find the most similar text chunk embeddings.
4.  Retrieves the actual text content and metadata (like source document name) for these chunks using a pre-generated mapping file stored on GCS.
5.  Returns a list of relevant chunks, which can then be used to augment the context for an LLM.

This service plays a critical role in providing factual, Aletheia-Fidelity-Constraint-aligned information to the AI agent.

## 2. Initialization & Dependencies (`_init_rag_dependencies_once`)

Upon module load (or first call if lazy loading was implemented differently), the service attempts to initialize its dependencies:
*   **Vertex AI SDK:** Ensures `vertexai.init()` is called with the project ID and region.
*   **Embedding Model:** Loads the `textembedding-gecko@003` model via `TextEmbeddingModel.from_pretrained()`.
*   **ID-to-Chunk Details Map:** Downloads and parses a JSON file (`id_to_chunk_details_map.json`) from a GCS bucket. This map is generated by the RAG pipeline (`scripts/rag_pipeline/01_process_and_embed_docs.py`) and contains the mapping from chunk IDs (returned by Vector Search) to their actual text, source document, and other metadata.
*   **Vertex AI Index Endpoint Client:** Initializes a client for the deployed RAG Index Endpoint using its ID/name.

**Required Environment Variables for Initialization:**
*   `GCP_PROJECT_ID`
*   `GCP_REGION`
*   `RAG_GCS_BUCKET_NAME` (for the chunk map)
*   `CHUNK_MAP_GCS_OBJECT_NAME` (path to chunk map in the bucket)
*   `VERTEX_AI_INDEX_ENDPOINT_ID` (Full resource name or numeric ID of the Index Endpoint)
*   `VERTEX_AI_DEPLOYED_INDEX_ID` (User-defined ID of the deployed index on that endpoint)

If these are not set or initialization fails, the RAG service will log errors and may not function correctly.

## 3. Core Function

### `async def retrieve_relevant_chunks(...)`

*   **Signature:**
    ```python
    async def retrieve_relevant_chunks(
        query_text: str,
        top_k: int = 3
    ) -> List[Dict[str, Any]]:
    ```
*   **Parameters:**
    *   `query_text: str`: The natural language query for which to find relevant information.
    *   `top_k: int`: The maximum number of relevant chunks to return.
*   **Process:**
    1.  **Generate Query Embedding:** Calls an internal helper `generate_embedding_for_query(query_text)` to convert the input text into a vector embedding using the configured Vertex AI embedding model.
    2.  **Query Vertex AI Vector Search:**
        *   Uses the initialized `IndexEndpoint` client's `find_neighbors_async()` method.
        *   Passes the query embedding, `top_k` (as `num_neighbors`), and the `deployed_index_id`.
        *   Optional filtering (e.g., by `source_document` namespace if configured in index data) can be added here if needed.
    3.  **Map IDs to Chunk Details:**
        *   The `find_neighbors_async()` response contains a list of neighbors, each with an `id` (the chunk ID) and `distance` (or similarity score).
        *   For each retrieved chunk ID, it looks up the corresponding details (actual text, source document name, other metadata) from the in-memory `_id_to_chunk_details_map`.
    4.  **Return Results:** Returns a list of dictionaries. Each dictionary represents a relevant chunk and includes:
        *   `id: str`: The unique ID of the chunk.
        *   `chunk_text: str`: The actual text content of the chunk.
        *   `source_document_name: str`: The name of the original document from which the chunk was extracted.
        *   `score: float`: The distance or similarity score from Vector Search.
        *   `metadata: Dict`: Other metadata associated with the chunk (e.g., `chunk_index_in_doc`, `start_index_in_doc`).
*   **Error Handling:** If dependencies are not initialized, query embedding fails, or the Vector Search query fails, it logs errors and returns an empty list.

## 4. `ALETHIA_FIDELITY_CONSTRAINT` Adherence

This service is designed to support the Alethia Fidelity Constraint by:
*   Retrieving chunks of text directly from the curated knowledge base.
*   Returning the `source_document_name` along with each chunk, allowing the AI agent or the application to cite the source of the information presented to the user.

## 5. `dynamous.ai` Design Contributions

*   **Technology Choice:** Endorsement of Vertex AI Vector Search for MVP to leverage a managed service for faster setup and scalability, aligning with `TA_Noah_MVP_v1.1`.
*   **Decoupled Data & Retrieval:** The strategy of storing chunk text and metadata separately (in the GCS JSON map) from the vector index itself (which only stores embeddings and IDs) is a common and flexible pattern for RAG systems. It allows the vector index to remain lean while rich metadata can be efficiently retrieved post-search.
*   **Clear Service Interface:** The `retrieve_relevant_chunks` function provides a simple and clear interface for the rest of the application to interact with the RAG system.
*   **Initialization and Configuration:** The service uses environment variables for configuration, making it adaptable to different environments. The one-time initialization `_init_rag_dependencies_once()` helps manage resources efficiently.
*   **Error Handling:** Basic error logging is included to aid in troubleshooting.

This RAG service provides the core retrieval capability needed to augment LLM responses with information from the project's curated knowledge base.
